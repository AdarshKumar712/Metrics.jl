<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Classification Metrics · Metrics.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Metrics.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../regression/">Regression Metrics</a></li><li class="is-active"><a class="tocitem" href>Classification Metrics</a><ul class="internal"><li><a class="tocitem" href="#Functions-1"><span>Functions</span></a></li><li><a class="tocitem" href="#Combined-Stats-1"><span>Combined Stats</span></a></li><li><a class="tocitem" href="#Utils-1"><span>Utils</span></a></li></ul></li><li><a class="tocitem" href="../nlp/">NLP Metrics</a></li><li><a class="tocitem" href="../cv/">CV Metrics</a></li><li><a class="tocitem" href="../rank/">Ranking Metrics</a></li><li><a class="tocitem" href="../utils/">Utils</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Classification Metrics</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Classification Metrics</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/AdarshKumar712/Metrics.jl/blob/master/docs/src/classification.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Classification-Metrics-1"><a class="docs-heading-anchor" href="#Classification-Metrics-1">Classification Metrics</a><a class="docs-heading-anchor-permalink" href="#Classification-Metrics-1" title="Permalink"></a></h1><p>This package allows you to use a variety of Classification metrics for the performance analysis of Classification models based on the provided <code>y_true</code> and <code>y_pred</code>. The metrics that you choose to evaluate your machine learning model is very important. Choice of metrics influences how the performance of machine learning algorithms is measured and compared. For most of these function, it is expected that the provided </p><h2 id="Functions-1"><a class="docs-heading-anchor" href="#Functions-1">Functions</a><a class="docs-heading-anchor-permalink" href="#Functions-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Metrics.binary_accuracy" href="#Metrics.binary_accuracy"><code>Metrics.binary_accuracy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">binary_accuracy(y_pred, y_true; threshold=0.5)</code></pre><p>Calculates Averaged Binary Accuracy based on <code>y_pred</code> and <code>y_true</code>. Argument <code>threshold</code> is used to specify the minimum predicted probability <code>y_pred</code> required to be labelled as <code>1</code>. Default value set as <code>0.5</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL68-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.categorical_accuracy" href="#Metrics.categorical_accuracy"><code>Metrics.categorical_accuracy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">categorical_accuracy(y_pred, y_true)</code></pre><p>Calculates Averaged Categorical Accuracy based on <code>y_pred</code> and <code>y_true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL78-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.cohen_kappa" href="#Metrics.cohen_kappa"><code>Metrics.cohen_kappa</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">cohen_kappa(y_pred, y_true)</code></pre><p>Measures the agreement between two raters (predicted and ground truth, here) who each classify N items into C mutually exclusive categories, using the observed data to calculate the probabilities of each observer randomly seeing each category. If the raters are in complete agreement then κ = 1. If there is no agreement among the raters other than what would be expected by chance, κ = 0.</p><p>Ref: <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen&#39;s Kappa</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL284-L292">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.confusion_matrix" href="#Metrics.confusion_matrix"><code>Metrics.confusion_matrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">confusion_matrix(y_pred, y_true)</code></pre><p>Function to create a confusion<em>matrix for classification problems based on provided `y</em>pred<code>and</code>y<em>true<code>. Expects</code>y</em>true`, to be onehot_enocded already.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL36-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.f_beta_score" href="#Metrics.f_beta_score"><code>Metrics.f_beta_score</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">f_beta_score(y_pred, y_true; β=1, avg_type=&quot;macro&quot;, sample_weights=nothing)</code></pre><p>Compute fbeta score. The F_beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</p><p><strong>Arguments</strong></p><ul><li><code>y_pred</code>: predicted values.</li><li><code>y_true</code>: ground truth values on the basis of which predicted values are to be assessed.</li><li><code>β=1</code>: the weight of precision in the combined score. If <code>β&lt;1</code>, more weight given to <code>precision</code>, while <code>β&gt;1</code> favors recall.</li><li><code>avg_type=&quot;macro&quot;</code>: Type of average to be used while calculating precision of multiclass models. Can take values as <code>macro</code>, <code>micro</code> and <code>weighted</code>. Default set to <code>macro</code>.</li><li><code>sample_weights</code>: Class weights to be provided when <code>avg_type</code> is set to <code>weighted</code>. Useful in case of imbalanced classes.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL216-L228">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.false_alarm_rate" href="#Metrics.false_alarm_rate"><code>Metrics.false_alarm_rate</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">false_alarm_rate(y_pred, y_true; avg_type=&quot;macro&quot;, sample_weights=nothing)</code></pre><p>Computes the false<em>alarm</em>raye of the predictions with respect to the labels as <code>1 - specificity(y_pred, y_true, avg_type, sample_weights)</code></p><p><strong>Arguments</strong></p><ul><li><code>y_pred</code>: predicted values.</li><li><code>y_true</code>: ground truth values on the basis of which predicted values are to be assessed.</li><li><code>avg_type=&quot;macro&quot;</code>: Type of average to be used while calculating precision of multiclass models. Can take values as <code>macro</code>, <code>micro</code> and <code>weighted</code>. Default set to <code>macro</code>.</li><li><code>sample_weights</code>: Class weights to be provided when <code>avg_type</code> is set to <code>weighted</code>. Useful in case of imbalanced classes.</li></ul><p>See also: <a href="#Metrics.specificity"><code>specificity</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL267-L279">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.precision" href="#Metrics.precision"><code>Metrics.precision</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">precision(y_pred, y_true; avg_type=&quot;macro&quot;, sample_weights=nothing)</code></pre><p>Computes the precision of the predictions with respect to the labels. </p><p><strong>Arguments</strong></p><ul><li><code>y_pred</code>: predicted values.</li><li><code>y_true</code>: ground truth values on the basis of which predicted values are to be assessed.</li><li><code>avg_type=&quot;macro&quot;</code>: Type of average to be used while calculating precision of multiclass models. Can take values as <code>macro</code>, <code>micro</code> and <code>weighted</code>. Default set to <code>macro</code>.</li><li><code>sample_weights</code>: Class weights to be provided when <code>avg_type</code> is set to <code>weighted</code>. Useful in case of imbalanced classes.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL149-L160">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.recall" href="#Metrics.recall"><code>Metrics.recall</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">recall(y_pred, y_true; avg_type=&quot;macro&quot;, sample_weights=nothing)</code></pre><p>Computes the recall of the predictions with respect to the labels.</p><p><strong>Arguments</strong></p><ul><li><code>y_pred</code>: predicted values.</li><li><code>y_true</code>: ground truth values on the basis of which predicted values are to be assessed.</li><li><code>avg_type=&quot;macro&quot;</code>: Type of average to be used while calculating precision of multiclass models. Can take values as <code>macro</code>, <code>micro</code> and <code>weighted</code>. Default set to <code>macro</code>.</li><li><code>sample_weights</code>: Class weights to be provided when <code>avg_type</code> is set to <code>weighted</code>. Useful in case of imbalanced classes.</li></ul><p>Aliases: <code>sensitivity</code> and <code>detection_rate</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL181-L193">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.sparse_categorical" href="#Metrics.sparse_categorical"><code>Metrics.sparse_categorical</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">sparse_categorical(y_pred, y_true)</code></pre><p>Calculated Sparse Categorical Accuracy based on <code>y_pred</code> and <code>y_true</code>. It evaluates the maximal true value is equal to the index of the maximal predicted value. Here, <code>y_true</code> is expected to provide only an integer (start from <code>0</code> index) as label for each data element (ie. not one hot encoded). </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL88-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.specificity" href="#Metrics.specificity"><code>Metrics.specificity</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">specificity(y_pred, y_true; avg_type=&quot;macro&quot;, sample_weights=nothing)</code></pre><p>Computes the specificity of the predictions with respect to the labels.</p><p><strong>Arguments</strong></p><ul><li><code>y_pred</code>: predicted values.</li><li><code>y_true</code>: ground truth values on the basis of which predicted values are to be assessed.</li><li><code>avg_type=&quot;macro&quot;</code>: Type of average to be used while calculating precision of multiclass models. Can take values as <code>macro</code>, <code>micro</code> and <code>weighted</code>. Default set to <code>macro</code>.</li><li><code>sample_weights</code>: Class weights to be provided when <code>avg_type</code> is set to <code>weighted</code>. Useful in case of imbalanced classes.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL236-L246">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.top_k_categorical" href="#Metrics.top_k_categorical"><code>Metrics.top_k_categorical</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">top_k_categorical(y_pred, y_true; k=3)</code></pre><p>Evaluates if the index of true value is equal to any of the indices of top k predicted values. Default value of <code>k</code> set to <code>3</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL98-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.top_k_sparse_categorical" href="#Metrics.top_k_sparse_categorical"><code>Metrics.top_k_sparse_categorical</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">top_k_sparse_categorical(y_pred, y_true; k=3)</code></pre><p>Evaluates if the true value is equal to any of the indices of top k predicted values. Default value of <code>k</code> set to <code>3</code>. Similar to <code>sparse_categorical</code>, expects the <code>y_true</code> to provide only an integer (start from <code>0</code> index) as label for each data element (ie. not one hot encoded).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL120-L124">source</a></section></article><h2 id="Combined-Stats-1"><a class="docs-heading-anchor" href="#Combined-Stats-1">Combined Stats</a><a class="docs-heading-anchor-permalink" href="#Combined-Stats-1" title="Permalink"></a></h2><p>There are some functions that return you the overall analysis of the model performance within a single function. They are:</p><article class="docstring"><header><a class="docstring-binding" id="Metrics.statsfromTFPN" href="#Metrics.statsfromTFPN"><code>Metrics.statsfromTFPN</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">statsfromTFPN(TP, TN, FP, FN)</code></pre><p>Computes statistics in case of binary classification or one-vs-all statsitics in case of multiclass classification.</p><p><strong>Arguments:</strong></p><ul><li><code>TP</code>: true positive values</li><li><code>TN</code>: true negative values</li><li><code>FP</code>: false positive values</li><li><code>FN</code>: false negative values</li></ul><p>Return the result stats as a dictionary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL303-L315">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.classwise_stats" href="#Metrics.classwise_stats"><code>Metrics.classwise_stats</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">classwise_stats(y_pred, y_true)</code></pre><p>Computes statistics for each of the class for multiclass classification based on provided <code>y_pred</code> and <code>y_true</code>.</p><p>Return the result stats as a dictionary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL330-L336">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.global_stats" href="#Metrics.global_stats"><code>Metrics.global_stats</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">global_stats(y_pred, y_true; avg_type=&quot;macro&quot;)</code></pre><p>Computes the overall statistics based on provided <code>y_pred</code> and <code>y_true</code>. <code>avg_type</code> allows to specify the type of average to be used while evaluating the stats. Currently, it can take values as &quot;macro&quot; or &quot;micro&quot;.</p><p>Return the result stats as a dictionary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL346-L352">source</a></section></article><h2 id="Utils-1"><a class="docs-heading-anchor" href="#Utils-1">Utils</a><a class="docs-heading-anchor-permalink" href="#Utils-1" title="Permalink"></a></h2><p>These are some utility functions to aid the overall performance analysis.</p><article class="docstring"><header><a class="docstring-binding" id="Metrics.bin_to_cat" href="#Metrics.bin_to_cat"><code>Metrics.bin_to_cat</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">bin_to_cat(y_pred, y_true)</code></pre><p>Function to convert binary type of data to categorical with two categories. Return <code>y_pred</code> and <code>y_true</code> of shape <code>(2, length(y_pred))</code> as tuple. Utility function to support performance metrics like <code>Precision</code>, <code>Recall</code> etc, where the function first need to be converted to categorical form before applying metric. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL13-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.TFPN" href="#Metrics.TFPN"><code>Metrics.TFPN</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">TFPN(y_pred, y_true)</code></pre><p>Returns <code>Confusion Matrix</code> and <code>True Positive</code>, <code>True Negative</code>, <code>False Positive</code> and <code>False Negative</code> for each class based on <code>y_pred</code> and <code>y_true</code>. Expects <code>y_true</code>, to be onehot_enocded already.  </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/6dc6fd6155afe551dd6424debdf7f034e68acb29/src/Classification.jl#LL48-L52">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../regression/">« Regression Metrics</a><a class="docs-footer-nextpage" href="../nlp/">NLP Metrics »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 25 March 2021 14:23">Thursday 25 March 2021</span>. Using Julia version 1.0.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
