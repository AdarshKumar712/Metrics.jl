<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · Metrics.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Metrics.jl</span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/AdarshKumar712/Metrics.jl/blob/master/docs/src/index.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Metrics.jl-1"><a class="docs-heading-anchor" href="#Metrics.jl-1">Metrics.jl</a><a class="docs-heading-anchor-permalink" href="#Metrics.jl-1" title="Permalink"></a></h1><ul><li><a href="#Metrics.Classwise_Stats-Tuple{Any,Any}"><code>Metrics.Classwise_Stats</code></a></li><li><a href="#Metrics.Perplexity-Tuple{Any,Any}"><code>Metrics.Perplexity</code></a></li><li><a href="#Metrics.TFPN-Tuple{Any,Any}"><code>Metrics.TFPN</code></a></li><li><a href="#Metrics._f_p_r_lcs-Tuple{Any,Any,Any}"><code>Metrics._f_p_r_lcs</code></a></li><li><a href="#Metrics._get_ngrams-Tuple{Any,Any}"><code>Metrics._get_ngrams</code></a></li><li><a href="#Metrics._get_word_ngrams-Tuple{Any,Any}"><code>Metrics._get_word_ngrams</code></a></li><li><a href="#Metrics._lcs-Tuple{Any,Any}"><code>Metrics._lcs</code></a></li><li><a href="#Metrics._len_lcs-Tuple{Any,Any}"><code>Metrics._len_lcs</code></a></li><li><a href="#Metrics._recon_lcs-Tuple{Any,Any}"><code>Metrics._recon_lcs</code></a></li><li><a href="#Metrics._split_into_words-Tuple{Any}"><code>Metrics._split_into_words</code></a></li><li><a href="#Metrics._union_lcs-Tuple{Any,Any}"><code>Metrics._union_lcs</code></a></li><li><a href="#Metrics.adjusted_r2_score-Tuple{Any,Any,Any}"><code>Metrics.adjusted_r2_score</code></a></li><li><a href="#Metrics.avg_precision"><code>Metrics.avg_precision</code></a></li><li><a href="#Metrics.binary_accuracy-Tuple{Any,Any}"><code>Metrics.binary_accuracy</code></a></li><li><a href="#Metrics.bleu_score-Tuple{Any,Any}"><code>Metrics.bleu_score</code></a></li><li><a href="#Metrics.categorical_accuracy-Tuple{Any,Any}"><code>Metrics.categorical_accuracy</code></a></li><li><a href="#Metrics.cohen_kappa-Tuple{Any,Any}"><code>Metrics.cohen_kappa</code></a></li><li><a href="#Metrics.confusion_matrix-Tuple{Any,Any}"><code>Metrics.confusion_matrix</code></a></li><li><a href="#Metrics.f_beta_score-Tuple{Any,Any}"><code>Metrics.f_beta_score</code></a></li><li><a href="#Metrics.false_alarm_rate-Tuple{Any,Any}"><code>Metrics.false_alarm_rate</code></a></li><li><a href="#Metrics.get_ngrams-Tuple{Any,Any}"><code>Metrics.get_ngrams</code></a></li><li><a href="#Metrics.global_stats-Tuple{Any,Any}"><code>Metrics.global_stats</code></a></li><li><a href="#Metrics.mae-Tuple{Any,Any}"><code>Metrics.mae</code></a></li><li><a href="#Metrics.male-Tuple{Any,Any}"><code>Metrics.male</code></a></li><li><a href="#Metrics.mse-Tuple{Any,Any}"><code>Metrics.mse</code></a></li><li><a href="#Metrics.msle-Tuple{Any,Any}"><code>Metrics.msle</code></a></li><li><a href="#Metrics.precision-Tuple{Any,Any}"><code>Metrics.precision</code></a></li><li><a href="#Metrics.r2_score-Tuple{Any,Any}"><code>Metrics.r2_score</code></a></li><li><a href="#Metrics.ranking_stats_k"><code>Metrics.ranking_stats_k</code></a></li><li><a href="#Metrics.recall-Tuple{Any,Any}"><code>Metrics.recall</code></a></li><li><a href="#Metrics.report_stats-Tuple{Any,Any}"><code>Metrics.report_stats</code></a></li><li><a href="#Metrics.rouge-Tuple{Any,Any}"><code>Metrics.rouge</code></a></li><li><a href="#Metrics.rouge_l_sentence_level-Tuple{Any,Any}"><code>Metrics.rouge_l_sentence_level</code></a></li><li><a href="#Metrics.rouge_l_summary_level-Tuple{Any,Any}"><code>Metrics.rouge_l_summary_level</code></a></li><li><a href="#Metrics.rouge_n-Tuple{Any,Any}"><code>Metrics.rouge_n</code></a></li><li><a href="#Metrics.sparse_categorical-Tuple{Any,Any}"><code>Metrics.sparse_categorical</code></a></li><li><a href="#Metrics.specificity-Tuple{Any,Any}"><code>Metrics.specificity</code></a></li><li><a href="#Metrics.statsfromTFPN-NTuple{4,Any}"><code>Metrics.statsfromTFPN</code></a></li><li><a href="#Metrics.top_k_categorical-Tuple{Any,Any}"><code>Metrics.top_k_categorical</code></a></li><li><a href="#Metrics.top_k_sparse_categorical-Tuple{Any,Any}"><code>Metrics.top_k_sparse_categorical</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="Metrics.Classwise_Stats-Tuple{Any,Any}" href="#Metrics.Classwise_Stats-Tuple{Any,Any}"><code>Metrics.Classwise_Stats</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">classwise_stats(y_pred, y_true)</code></pre><p>Computes statistics for each of the class for multiclass classification based on provided <code>y_pred</code> and <code>y_true</code>.</p><p>Return the result stats as a dictionary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL317-L323">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.Perplexity-Tuple{Any,Any}" href="#Metrics.Perplexity-Tuple{Any,Any}"><code>Metrics.Perplexity</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">perplexity(y_pred, y_true)</code></pre><p>Returns the exponentiation of <code>crossentropy</code> based on <code>y_pred</code> and <code>y_true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/nlp_metrics.jl#LL4-L8">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.TFPN-Tuple{Any,Any}" href="#Metrics.TFPN-Tuple{Any,Any}"><code>Metrics.TFPN</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TFPN(y_pred, y_true)</code></pre><p>Returns <code>Confusion Matrix</code> and <code>True Positive</code>, <code>True Negative</code>, <code>False Positive</code> and <code>False Negative</code> for each class based on <code>y_pred</code> and <code>y_true</code>. Expects <code>y_true</code>, to be onehot_enocded already.  </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL35-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics._f_p_r_lcs-Tuple{Any,Any,Any}" href="#Metrics._f_p_r_lcs-Tuple{Any,Any,Any}"><code>Metrics._f_p_r_lcs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">_f_p_r_lcs(llcs, m, n)</code></pre><p>Computes the LCS-based F-measure score</p><p><strong>Arguments:</strong></p><ul><li><code>llcs</code>: Length of LCS</li><li><code>m</code>: number of words in reference summary</li><li><code>n</code>: number of words in candidate summary</li></ul><p>Source: (http://research.microsoft.com/en-us/um/people/cyl/download/papers/rouge-working-note-v1.3.1.pdf)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL152-L163">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics._get_ngrams-Tuple{Any,Any}" href="#Metrics._get_ngrams-Tuple{Any,Any}"><code>Metrics._get_ngrams</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">_get_ngrams(n, text)</code></pre><p>Calcualtes n-grams. Returns a set of n-grams.</p><p><strong>Arguments:</strong></p><ul><li><code>n</code>: provide which n-grams to calculate</li><li><code>text</code>: An array of tokens</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL4-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics._get_word_ngrams-Tuple{Any,Any}" href="#Metrics._get_word_ngrams-Tuple{Any,Any}"><code>Metrics._get_word_ngrams</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">_get_word_ngrams(n, sentences)</code></pre><p>Calculates word n-grams for multiple sentences.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL39-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics._lcs-Tuple{Any,Any}" href="#Metrics._lcs-Tuple{Any,Any}"><code>Metrics._lcs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">_lcs(x, y)</code></pre><p>Utility function to compute the length of the longest common subsequence (lcs) between two strings. The implementation below uses a DP programming algorithm and runs in O(nm) time where n = len(x) and m = len(y).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL52-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics._len_lcs-Tuple{Any,Any}" href="#Metrics._len_lcs-Tuple{Any,Any}"><code>Metrics._len_lcs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">_len_lcs(x, y)</code></pre><p>Returns the length of the Longest Common Subsequence between sequences x and y.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL76-L80">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics._recon_lcs-Tuple{Any,Any}" href="#Metrics._recon_lcs-Tuple{Any,Any}"><code>Metrics._recon_lcs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">_recons_lcs(x, y)</code></pre><p>Returns the Longest Subsequence between x and y.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL87-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics._split_into_words-Tuple{Any}" href="#Metrics._split_into_words-Tuple{Any}"><code>Metrics._split_into_words</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">_split_into_words(sentences)</code></pre><p>Splits multiple sentences into words and flattens the result</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL23-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics._union_lcs-Tuple{Any,Any}" href="#Metrics._union_lcs-Tuple{Any,Any}"><code>Metrics._union_lcs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">_union_lcs(evaluated_sentences, reference_sentence)</code></pre><p>Returns LCS<em>u(r</em>i, C) which is the LCS score of the union longest common subsequence between reference sentence ri and candidate summary C.</p><p><strong>Arguments:</strong></p><ul><li><code>evaluated_sentences</code>: the sentences that have been picked by the summarizer</li><li><code>reference_sentence</code>: one of the sentences in the reference summaries</li></ul><p>For example, if r<em>i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r</em>i and c1 is “w1 w2” and the longest common subsequence of r<em>i and c2 is “w1 w3 w5”. The union longest common subsequence of r</em>i, c1, and c2 is “w1 w2 w3 w5” and LCS<em>u(r</em>i, C) = 4/5.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL209-L220">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.adjusted_r2_score-Tuple{Any,Any,Any}" href="#Metrics.adjusted_r2_score-Tuple{Any,Any,Any}"><code>Metrics.adjusted_r2_score</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">adjusted_r2_score(y_pred, y_true, n)</code></pre><p>Modified version of <code>r2_score</code> that has been adjusted for the number of predictors in the model. Here the argument <code>n</code> is for the number of predictors(or independent variables in X). </p><p>See also: <a href="#Metrics.r2_score-Tuple{Any,Any}"><code>r2_score</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Regression.jl#LL59-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.avg_precision" href="#Metrics.avg_precision"><code>Metrics.avg_precision</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">avg_precision(y_rec, y_rel, k = 10)</code></pre><p>Evaluates how much of the relevant documents are concentrated in the highest ranked predictions. </p><p>Calculated as     ∑(Recall@i - Recall@i-1)* Precision@i for i = (1, 2, 3....k)</p><p>Here, <code>y_rec</code> are predicted probabilities for recommendation and <code>y_rel</code> defines as <code>1</code> if particular result is relevant, else <code>0</code>. The shape of <code>y_rec</code> is expected to be (1, N_elements)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Ranking_n_Statistical.jl#LL32-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.binary_accuracy-Tuple{Any,Any}" href="#Metrics.binary_accuracy-Tuple{Any,Any}"><code>Metrics.binary_accuracy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">binary_accuracy(y_pred, y_true; threshold=0.5)</code></pre><p>Calculates Averaged Binary Accuracy based on <code>y_pred</code> and <code>y_true</code>. Argument <code>threshold</code> is used to specify the minimum predicted probability <code>y_pred</code> required to be labelled as <code>1</code>. Default value set as <code>0.5</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL55-L59">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.bleu_score-Tuple{Any,Any}" href="#Metrics.bleu_score-Tuple{Any,Any}"><code>Metrics.bleu_score</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">bleu_score(reference_corpus, translation_corpus; max_order=4, smooth=false)</code></pre><p>Computes BLEU score of translated segments against one or more references. Returns the <code>BLEU score</code>, <code>n-gram precisions</code>, <code>brevity penalty</code>,  geometric mean of n-gram precisions, translation<em>length and  reference</em>length</p><p><strong>Arguments</strong></p><ul><li><code>reference_corpus</code>: list of lists of references for each translation. Each reference should be tokenized into a list of tokens.</li><li><code>translation_corpus</code>: list of translations to score. Each translation should be tokenized into a list of tokens.</li><li><code>max_order</code>: maximum n-gram order to use when computing BLEU score. </li><li><code>smooth=false</code>: whether or not to apply. Lin et al. 2004 smoothing.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/bleu.jl#LL32-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.categorical_accuracy-Tuple{Any,Any}" href="#Metrics.categorical_accuracy-Tuple{Any,Any}"><code>Metrics.categorical_accuracy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">categorical_accuracy(y_pred, y_true)</code></pre><p>Calculates Averaged Categorical Accuracy based on <code>y_pred</code> and <code>y_true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL65-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.cohen_kappa-Tuple{Any,Any}" href="#Metrics.cohen_kappa-Tuple{Any,Any}"><code>Metrics.cohen_kappa</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">cohen_kappa(y_pred, y_true)</code></pre><p>Measures the agreement between two raters (predicted and ground truth, here) who each classify N items into C mutually exclusive categories, using the observed data to calculate the probabilities of each observer randomly seeing each category. If the raters are in complete agreement then κ = 1. If there is no agreement among the raters other than what would be expected by chance, κ = 0.</p><p>Ref: <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen&#39;s Kappa</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL271-L279">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.confusion_matrix-Tuple{Any,Any}" href="#Metrics.confusion_matrix-Tuple{Any,Any}"><code>Metrics.confusion_matrix</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">confusion_matrix(y_pred, y_true)</code></pre><p>Function to create a confusion<em>matrix for classification problems based on provided `y</em>pred<code>and</code>y<em>true<code>. Expects</code>y</em>true`, to be onehot_enocded already.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL23-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.f_beta_score-Tuple{Any,Any}" href="#Metrics.f_beta_score-Tuple{Any,Any}"><code>Metrics.f_beta_score</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">f_beta_score(y_pred, y_true; β=1, avg_type=&quot;macro&quot;, sample_weights=nothing)</code></pre><p>Compute fbeta score. The F_beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</p><p><strong>Arguments</strong></p><ul><li><code>y_pred</code>: predicted values.</li><li><code>y_true</code>: ground truth values on the basis of which predicted values are to be assessed.</li><li><code>β=1</code>: the weight of precision in the combined score. If <code>β&lt;1</code>, more weight given to <code>precision</code>, while <code>β&gt;1</code> favors recall.</li><li><code>avg_type=&quot;macro&quot;</code>: Type of average to be used while calculating precision of multiclass models. Can take values as <code>macro</code>, <code>micro</code> and <code>weighted</code>. Default set to <code>macro</code>.</li><li><code>sample_weights</code>: Class weights to be provided when <code>avg_type</code> is set to <code>weighted</code>. Useful in case of imbalanced classes.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL203-L215">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.false_alarm_rate-Tuple{Any,Any}" href="#Metrics.false_alarm_rate-Tuple{Any,Any}"><code>Metrics.false_alarm_rate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">false_alarm_rate(y_pred, y_true; avg_type=&quot;macro&quot;, sample_weights=nothing)</code></pre><p>Computes the false<em>alarm</em>raye of the predictions with respect to the labels as <code>1 - specificity(y_pred, y_true, avg_type, sample_weights)</code></p><p><strong>Arguments</strong></p><ul><li><code>y_pred</code>: predicted values.</li><li><code>y_true</code>: ground truth values on the basis of which predicted values are to be assessed.</li><li><code>avg_type=&quot;macro&quot;</code>: Type of average to be used while calculating precision of multiclass models. Can take values as <code>macro</code>, <code>micro</code> and <code>weighted</code>. Default set to <code>macro</code>.</li><li><code>sample_weights</code>: Class weights to be provided when <code>avg_type</code> is set to <code>weighted</code>. Useful in case of imbalanced classes.</li></ul><p>See also: <a href="#Metrics.specificity-Tuple{Any,Any}"><code>specificity</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL254-L266">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.get_ngrams-Tuple{Any,Any}" href="#Metrics.get_ngrams-Tuple{Any,Any}"><code>Metrics.get_ngrams</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">get_ngrams(segment, max_order)</code></pre><p>Extracts all n-grams upto a given maximum order from an input segment. Returns the counter containing all n-grams upto max_order in segment with a count of how many times each n-gram occurred.</p><p><strong>Arguments</strong></p><ul><li><code>segment</code>: text segment from which n-grams will be extracted.</li><li><code>max_order</code>: maximum length in tokens of the n-grams returned by this methods.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/bleu.jl#LL6-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.global_stats-Tuple{Any,Any}" href="#Metrics.global_stats-Tuple{Any,Any}"><code>Metrics.global_stats</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">global_stats(y_pred, y_true; avg_type=&quot;macro&quot;)</code></pre><p>Computes the overall statistics based on provided <code>y_pred</code> and <code>y_true</code>. <code>avg_type</code> allows to specify the type of average to be used while evaluating the stats. Currently, it can take values as &quot;macro&quot; or &quot;micro&quot;.</p><p>Return the result stats as a dictionary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL333-L339">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.mae-Tuple{Any,Any}" href="#Metrics.mae-Tuple{Any,Any}"><code>Metrics.mae</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mae(y_pred, y_true)</code></pre><p>Mean Absolute Error. Calculated as <code>sum(|y_true .- y_pred|) / length(y_true)</code> based on provided <code>y_pred</code> and <code>y_true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Regression.jl#LL3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.male-Tuple{Any,Any}" href="#Metrics.male-Tuple{Any,Any}"><code>Metrics.male</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">male(y_pred, y_true)</code></pre><p>Mean Absolute Logarithmic Error. Calculated as <code>sum(|log.(y_true) .- log.(y_pred)|) / length(y_true)</code> based on provided <code>y_pred</code> and <code>y_true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Regression.jl#LL23-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.mse-Tuple{Any,Any}" href="#Metrics.mse-Tuple{Any,Any}"><code>Metrics.mse</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mse(y_pred, y_true)</code></pre><p>Mean Squared Error. Calculated as <code>sum((y_true .- y_pred).^2) / length(y_true)</code> based on provided <code>y_pred</code> and <code>y_true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Regression.jl#LL13-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.msle-Tuple{Any,Any}" href="#Metrics.msle-Tuple{Any,Any}"><code>Metrics.msle</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">msle(y_pred, y_true)</code></pre><p>Mean Absolute Logarithmic Error. Calculated as <code>sum((log.(y_true) .- log.(y_pred)).^2) / length(y_true)</code> based on provided <code>y_pred</code> and <code>y_true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Regression.jl#LL33-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.precision-Tuple{Any,Any}" href="#Metrics.precision-Tuple{Any,Any}"><code>Metrics.precision</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">precision(y_pred, y_true; avg_type=&quot;macro&quot;, sample_weights=nothing)</code></pre><p>Computes the precision of the predictions with respect to the labels. </p><p><strong>Arguments</strong></p><ul><li><code>y_pred</code>: predicted values.</li><li><code>y_true</code>: ground truth values on the basis of which predicted values are to be assessed.</li><li><code>avg_type=&quot;macro&quot;</code>: Type of average to be used while calculating precision of multiclass models. Can take values as <code>macro</code>, <code>micro</code> and <code>weighted</code>. Default set to <code>macro</code>.</li><li><code>sample_weights</code>: Class weights to be provided when <code>avg_type</code> is set to <code>weighted</code>. Useful in case of imbalanced classes.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL136-L147">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.r2_score-Tuple{Any,Any}" href="#Metrics.r2_score-Tuple{Any,Any}"><code>Metrics.r2_score</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">r2_score(y_pred, y_true)</code></pre><p>Calculates the r2 (Coefficient of Determination) score for the provided <code>y_pred</code> and <code>y_true</code>. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a r2_score of <code>0.0</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Regression.jl#LL44-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.ranking_stats_k" href="#Metrics.ranking_stats_k"><code>Metrics.ranking_stats_k</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ranking_stats_k(y_rec, y_rel, k = 10)</code></pre><p>Evaluates the relevancy of top k recommendations using <code>precison@k</code>, <code>recall@k</code> and <code>f1_score@k</code>. Returns result as a <code>Dict</code>.</p><p>Here, <code>y_rec</code> are predicted probabilities for recommendation and <code>y_rel</code> defines as <code>1</code> if particular result is relevant, else <code>0</code>. The shape of <code>y_rec</code> is expected to be (1, N_elements).&lt;br&gt;</p><p><code>precison_k</code> is evaluated as <code>Recommended_items_that_are_relevant / Total_Recommended_items</code> <code>recall_l</code> is evaluated as <code>Recommended_items_that_are_relevant / Total_Relevant_items</code> <code>f1_k</code> is evaluated as <code>2 * Recommended_items_that_are_relevant / (Total_Recommended_items + Total_Relevant_items)</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Ranking_n_Statistical.jl#LL3-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.recall-Tuple{Any,Any}" href="#Metrics.recall-Tuple{Any,Any}"><code>Metrics.recall</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">recall(y_pred, y_true; avg_type=&quot;macro&quot;, sample_weights=nothing)</code></pre><p>Computes the recall of the predictions with respect to the labels.</p><p><strong>Arguments</strong></p><ul><li><code>y_pred</code>: predicted values.</li><li><code>y_true</code>: ground truth values on the basis of which predicted values are to be assessed.</li><li><code>avg_type=&quot;macro&quot;</code>: Type of average to be used while calculating precision of multiclass models. Can take values as <code>macro</code>, <code>micro</code> and <code>weighted</code>. Default set to <code>macro</code>.</li><li><code>sample_weights</code>: Class weights to be provided when <code>avg_type</code> is set to <code>weighted</code>. Useful in case of imbalanced classes.</li></ul><p>Aliases: <code>sensitivity</code> and <code>detection_rate</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL168-L180">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.report_stats-Tuple{Any,Any}" href="#Metrics.report_stats-Tuple{Any,Any}"><code>Metrics.report_stats</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">report_stats(y_pred, y_true; classwise_stats=true, avg_type=&quot;macro&quot;, sample_weights=nothing)</code></pre><p>A utility function that prints the statistics summary of the model based on provided <code>y_pred</code> and <code>y_true</code>. </p><p><strong>Arguments:</strong></p><ul><li><code>y_pred</code>: predicted values  </li><li><code>y_true</code>: ground truth values on the basis of which predicted values are to be assessed.</li><li><code>classwise_stats=true</code>: if set <code>true</code>, prints classwise stats along with global stats.</li><li><code>avg_type=&quot;macro&quot;</code>: Type of average to be used while calculating precision of multiclass models. Can take values as <code>macro</code>, <code>micro</code> and <code>weighted</code>. Default set to <code>macro</code>.</li><li><code>sample_weights</code>: Class weights to be provided when <code>avg_type</code> is set to <code>weighted</code>. Useful in case of imbalanced classes.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/utils.jl#LL4-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.rouge-Tuple{Any,Any}" href="#Metrics.rouge-Tuple{Any,Any}"><code>Metrics.rouge</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">rouge(hypotheses, references)</code></pre><p>Calculates average rouge scores for a list of hypotheses and references.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL279-L283">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.rouge_l_sentence_level-Tuple{Any,Any}" href="#Metrics.rouge_l_sentence_level-Tuple{Any,Any}"><code>Metrics.rouge_l_sentence_level</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">rouge_l_sentence_level(evaluated_sentences, reference_sentences)</code></pre><p>Computes ROUGE-L (sentence level) of two text collections of sentences.</p><p>Calculated according to:   R<em>lcs = LCS(X,Y)/m,   P</em>lcs = LCS(X,Y)/n,   F<em>lcs = ((1 + beta^2)*R</em>lcs*P<em>lcs) / (R</em>lcs + (beta^2) * P_lcs)</p><p>where:   X = reference summary   Y = Candidate summary   m = length of reference summary   n = length of candidate summary</p><p><strong>Argumnets:</strong></p><ul><li><code>evaluated_sentences</code>: the sentences that have been picked by the summarizer</li><li><code>reference_sentences</code>: the sentences from the referene set</li></ul><p>Source: (http://research.microsoft.com/en-us/um/people/cyl/download/papers/rouge-working-note-v1.3.1.pdf)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL174-L195">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.rouge_l_summary_level-Tuple{Any,Any}" href="#Metrics.rouge_l_summary_level-Tuple{Any,Any}"><code>Metrics.rouge_l_summary_level</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">rouge_l_summary_level(evaluated_sentences, reference_sentences)</code></pre><p>Computes ROUGE-L (summary level) of two text collections of sentences.</p><p>Calculated according to:   R<em>lcs = SUM(1, u)[LCS&lt;union&gt;(r</em>i,C)]/m   P<em>lcs = SUM(1, u)[LCS&lt;union&gt;(r</em>i,C)]/n   F<em>lcs = ((1 + beta^2)*R</em>lcs*P<em>lcs) / (R</em>lcs + (beta^2) * P_lcs)</p><p>where:   SUM(i,u) = SUM from i through u   u = number of sentences in reference summary   C = Candidate summary made up of v sentences   m = number of words in reference summary   n = number of words in candidate summary</p><p><strong>Arguments:</strong></p><ul><li><code>evaluated_sentences</code>: the sentences that have been picked by the summarizer</li><li><code>reference_sentence</code>: the sentences in the reference summaries</li></ul><p>Source: (http://research.microsoft.com/en-us/um/people/cyl/download/papers/rouge-working-note-v1.3.1.pdf)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL241-L264">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.rouge_n-Tuple{Any,Any}" href="#Metrics.rouge_n-Tuple{Any,Any}"><code>Metrics.rouge_n</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">rouge_n(evaluated_sentences, reference_sentences; n=2)</code></pre><p>Computes ROUGE-N of two text collections of sentences. Returns f1, precision, recall for ROUGE-N.</p><p><strong>Arguments:</strong></p><ul><li><code>evaluated_sentences</code>: the sentences that have been picked by the summarizer</li><li><code>reference_sentences</code>: the sentences from the referene set</li><li><code>n</code>: size of ngram.  Defaults to 2.</li></ul><p>Source: (http://research.microsoft.com/en-us/um/people/cyl/download/   papers/rouge-working-note-v1.3.1.pdf)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/NLP_Metrics/rouge.jl#LL111-L123">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.sparse_categorical-Tuple{Any,Any}" href="#Metrics.sparse_categorical-Tuple{Any,Any}"><code>Metrics.sparse_categorical</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">sparse_categorical(y_pred, y_true)</code></pre><p>Calculated Sparse Categorical Accuracy based on <code>y_pred</code> and <code>y_true</code>. It evaluates the maximal true value is equal to the index of the maximal predicted value. Here, <code>y_true</code> is expected to provide only an integer as label for each data element (ie. not one hot encoded). </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL75-L79">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.specificity-Tuple{Any,Any}" href="#Metrics.specificity-Tuple{Any,Any}"><code>Metrics.specificity</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">specificity(y_pred, y_true; avg_type=&quot;macro&quot;, sample_weights=nothing)</code></pre><p>Computes the specificity of the predictions with respect to the labels.</p><p><strong>Arguments</strong></p><ul><li><code>y_pred</code>: predicted values.</li><li><code>y_true</code>: ground truth values on the basis of which predicted values are to be assessed.</li><li><code>avg_type=&quot;macro&quot;</code>: Type of average to be used while calculating precision of multiclass models. Can take values as <code>macro</code>, <code>micro</code> and <code>weighted</code>. Default set to <code>macro</code>.</li><li><code>sample_weights</code>: Class weights to be provided when <code>avg_type</code> is set to <code>weighted</code>. Useful in case of imbalanced classes.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL223-L233">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.statsfromTFPN-NTuple{4,Any}" href="#Metrics.statsfromTFPN-NTuple{4,Any}"><code>Metrics.statsfromTFPN</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">statsfromTFPN(TP, TN, FP, FN)</code></pre><p>Computes statistics in case of binary classification or one-vs-all statsitics in case of multiclass classification.</p><p><strong>Arguments:</strong></p><ul><li><code>TP</code>: true positive values</li><li><code>TN</code>: true negative values</li><li><code>FP</code>: false positive values</li><li><code>FN</code>: false negative values</li></ul><p>Return the result stats as a dictionary.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL290-L302">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.top_k_categorical-Tuple{Any,Any}" href="#Metrics.top_k_categorical-Tuple{Any,Any}"><code>Metrics.top_k_categorical</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">top_k_categorical(y_pred, y_true; k=3)</code></pre><p>Evaluates if the index of true value is equal to any of the indices of top k predicted values. Default value of <code>k</code> set to <code>3</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL85-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metrics.top_k_sparse_categorical-Tuple{Any,Any}" href="#Metrics.top_k_sparse_categorical-Tuple{Any,Any}"><code>Metrics.top_k_sparse_categorical</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">top_k_sparse_categorical(y_pred, y_true; k=3)</code></pre><p>Evaluates if the true value is equal to any of the indices of top k predicted values. Default value of <code>k</code> set to <code>3</code>. Similar to <code>sparse_categorical</code>, expects the <code>y_true</code> to provide only an integer as label for each data element (ie. not one hot encoded).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/AdarshKumar712/Metrics.jl/blob/5dd8387d74b0daf57e9e706b97586ece5d21075d/src/Classification.jl#LL107-L111">source</a></section></article></article></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 9 May 2020 16:24">Saturday 9 May 2020</span>. Using Julia version 1.0.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
